# Issues
- Shrinking the data down from 42 GB to something a little more manageable for my weakling windows machines took a little effort, seeing as I only wanted to unzip and grab part of the CSV. In this way, I created a sort of batched reader to only grab as many GB as I ask it to. This took 449840 ms for 22 GB (just for unzipping to CSV) on my desktop. On my laptop, I was only able to unzip around 25 GB worth of the csv before the program crashed (this was before I wrote in the ability to specify how many GB I wanted). This left my csv unfinished, causing my last row to be halved in terms of number of columns. The batched reader also, by default, is likely to cut off the last batch because the likelihood is very low that 22 GB unzipped is perfectly aligned with the last row it grabs. This is ok since I was planning on not using the entire dataset anyway, but in the future I may try to optimize this.
- There were a few columns that did not need to be collected, including the steam_china columns that provided me with little interesting information (and unclean columns) along with the recommendation id's and app id's, so dropping these helped save on size later down the line.
- There are many columns that provide an opportunity to reduce the size of the data with little to no lossiness, seeing as they were initially 64 bits and many of the values could instead be 32 bits in order to shrink the size of the data. I played around with changed what columns I saw fit for this optimization, and found a couple columns with values that appeared to be the result of some unsigned int conversion to signed integers (4.2 billion). I picked these columns out specifically to find they dealth with the voting on comments, which would make sense, as some comments were likely downvoted to have a negative rating. I used a simple 2's compliment wraparound with a cast to a signed 32 bit integer to fix these cases. The rest of the data I was able to make 32 bit as well, save for the author_steamid which I may try to make unique in the future (harder to do with batching). I also converted the appid column to a string, seeing as the appid's were quite small and I figured I would not be using them in a meaningful way, number-wise. My thought it to maybe grab all the appid's into some sort of json that will allow me to populate rows with game names after querying the data on just appid, however this reduces the ability to query based on game title if I don't know what the appid is.
- I continuously received the following error: `Error processing row: In CSV column #23: CSV conversion error to null: invalid value '浙江'`, and spent a long time trying to parse the data differently in order to avoid it. My theory after a while was that the pyarrow csv_reader was inferring this row to be Null for the first 100,000+ values, and then would consistenly stumble upon the same entry that looked like a string. I initially tried to fix this by going inside of the batched loop and dropping that row entirely, but the record_batch gets populated with the pyarrow table for each chunk, so it had to be outside of the loop. Once I realized this, I was able to throw in a few extra options to the csv_reader that allowed me to configure that column to be a string right off the bat, which removed any issue.